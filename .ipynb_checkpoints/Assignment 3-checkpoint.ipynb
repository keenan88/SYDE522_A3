{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 522 Assignment 3\n",
    "## Backpropagation and Multilayer Perceptrons\n",
    "\n",
    "### Due: Monday Nov 6 at 11:59pm\n",
    "\n",
    "As with all the assignments in this course, this assignment is structured as a Jupyter Notebook and uses Python.  If you do not have Python and Jupyter Notebook installed, the easiest method is to download and install Anaconda https://www.anaconda.com/download.  There is a quick tutorial for running Jupyter Notebook from within Anacoda at https://docs.anaconda.com/free/anaconda/getting-started/hello-world/#python-exercise-jupyter under \"Run Python in a Jupyter Notebook\"\n",
    "\n",
    "Implement your assignment directly in the Jupyter notebook and submit your resulting Jupyter Notebook file using Learn.\n",
    "\n",
    "While you are encouraged to talk about the assignment with your classmates, you must write and submit your own assignment.  Directly copying someone else's assignment and changing a few small things here and there does not count as writing your own assignment.\n",
    "\n",
    "Make sure to label the axes on all of your graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) [1 mark]** The following code generates the nested circles dataset that we have used in class before.\n",
    "\n",
    "```python\n",
    "x, y = sklearn.datasets.make_circles(n_samples=100, shuffle=True, noise=0.1, random_state=0, factor=0.3)\n",
    "```\n",
    "\n",
    "As before, you can split this into training and test data\n",
    "```python\n",
    "import sklearn.model_selection\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y, test_size=0.2, shuffle=True, random_state=0\n",
    ")\n",
    "```\n",
    "\n",
    "To classify this data using a multi-layer perceptron trained using backprop, we can use the built-in implementation in `sklearn`.\n",
    "\n",
    "```python\n",
    "mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(20,), # one hidden layer with 20 features \n",
    "                                          activation='relu',        # rectified linear\n",
    "                                          learning_rate_init=1e-2,  # learning rate\n",
    "                                          max_iter=1000,            # number of iterations\n",
    "                                          early_stopping=True,      # stop training if validation data gets worse\n",
    "                                          random_state=0)           # random number seed for initialization\n",
    "```\n",
    "\n",
    "To train the model, use\n",
    "```python\n",
    "mlp.fit(x_train, y_train)\n",
    "```\n",
    "\n",
    "To determine the outputs on your testing data `x_test`, use\n",
    "```python\n",
    "output = mlp.predict(x_test)\n",
    "```\n",
    "\n",
    "Train your the network using 80% of the data as training data and test it on the other 20%.  Compute the Root Mean Squared Error and report it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) [1 mark]** For the model you trained in part a), plot the output for a grid of inputs between -2 and 2.  This can be done using similar code as used in the last assignment:\n",
    "```python\n",
    "extent = (-2, 2, -2, 2)\n",
    "G = 200\n",
    "XX, YY = np.meshgrid(np.linspace(extent[2],extent[3],G), np.linspace(extent[0],extent[1],G))\n",
    "pts = np.vstack([YY.flatten(), XX.flatten()]).T\n",
    "output_pts = mlp.predict(pts)\n",
    "im = plt.imshow(output_pts.reshape((G,G)).T, vmin=0, vmax=1, cmap='RdBu',\n",
    "                extent=(extent[0], extent[1], extent[3], extent[2]))\n",
    "plt.scatter(x[:,0], x[:,1], c=np.where(y==1, 'blue', 'red'))\n",
    "```\n",
    "\n",
    "Has the network learned to classify the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) [1 mark]** Repeat part a) but reduce the network size so that there are only 10 features (i.e. 10 neurons in the hidden layer).  Report the RMSE and generate the same plot as in part b).  Has the network learned to classify the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) [1 mark]** Repeat part a) but for the following different number of features: `[5, 10, 15, 20, 25, 30, 35, 40, 45, 50]`.  For each number of features, repeat the process 10 times and compute the average RMSE over those 10 times.  Note that you will have to change the `random_state=0` parameter each time, in both the `MLPRegressor` and the `train_test_split` code.  For example, if you do this in a for loop `for i in range(10):` then you would set `random_state=i`.  \n",
    "\n",
    "Generate a plot showing how the average RMSE changes as you adjust the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) [1 mark]** Repeat part d) but add an extra layer of features (i.e. an extra layer inside the network).  Do this by setting `hidden_layer_sizes`.  In the previous example, we set it t `(20,)` to generate one internal layer of 20 features.  To have two internal layers both having 20 features, set it to `(20,20)`.  For this question, use the same number of features in both layers (i.e. try it with `(5,5)`, then `(10,10)`, then `(15, 15)` and so on up to `(50,50)`).  Generate a plot showing how the average RMSE changes as you change these numbers of features.\n",
    "\n",
    "How does your result in part (e) compare to your result in part (d)?  What does this indicate about how useful the second layer of features is for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) [1 mark]** Repeat part a) and b) but for this dataset:\n",
    "```python\n",
    "x, y = sklearn.datasets.make_circles(n_samples=100, shuffle=True, noise=0.1, random_state=0, factor=0.3)\n",
    "x[:,0]*=0.1\n",
    "```\n",
    "(i.e. exactly the same dataset, but with the `x` values scaled by 0.1)\n",
    "\n",
    "Report the RMSE and generate the output plot from part b).\n",
    "\n",
    "Is the accuracy better or worse on this scaled dataset, as compared to the original parts a) and b)?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) [1 mark]** `sklearn` has a tool for automatically rescaling data for you.  You can create a scaler as follows:\n",
    "\n",
    "```python\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "```\n",
    "\n",
    "and then you can transform the `X_train` and `X_test` with `scaler.transform(X_train)` and `scaler.transform(X_test)`.  You can even transform the `pts` used to create the output plot using `scaler.transform(pts)`.\n",
    "\n",
    "Repeat part f) but use the `scaler` to scale your data before using it.  Report the RMSE and generate the output plot from part b).  How does this accuracy compart to part f) and to the original part a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h) [1 mark]** Repeat parts a) and b) with the following dataset:\n",
    "\n",
    "```python\n",
    "x, y = sklearn.datasets.make_moons(n_samples=500, noise=0.05, random_state=0)\n",
    "```\n",
    "\n",
    "Try it both with and without the `scaler` from the part (g), and report the RMSE and generate the output plot both ways.  Should you use the `scaler` for this sort of data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) [1 mark]** Repeat parts a) and b) with the following dataset:\n",
    "\n",
    "```python\n",
    "x, y = sklearn.datasets.make_blobs(centers=[[-1, -1], [1, 1]], \n",
    "                                             cluster_std=[1, 1], \n",
    "                                             random_state=0, \n",
    "                                             n_samples=200, \n",
    "                                             n_features=2)\n",
    "```\n",
    "\n",
    "Try it both with and without the `scaler` from part(g), and report the RMSE and generate the output plot both ways.  Note that you will need to adjust the `extent = (-2, 2, -2, 2)` line so that the output plot covers the same range as the training data (try `(-4,4,-4,4)`).  Should you use the `scaler` for this sort of data?  Why or why not?  Why is the error for this dataset much larger than the error for the other datasets we done so far in this assignment?  Could you improve the accuracy by adjusting the network?  If so, what would you change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) [1 mark]** When using an MLP to do classification, we often don't really care what the exact numerical value of the output is: we just want to classify the input data into a particular category.  The `sklearn.neural_network.MLPClassifier` does this for us, training a separate output for each category (one-hot encoding) and then classifying based on which output is largest.  (Note: it also uses a slightly different Loss function, where the goal is to minimize classification error, rather than minimizing ${1 \\over 2}(y_{target}-y)^2$).\n",
    "\n",
    "You can use the `MLPClassifier` with the same parameters as the `MLPRegressor` we used in question 1.\n",
    "\n",
    "```\n",
    "mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(20,), # one hidden layer with 20 features \n",
    "                                           activation='relu',        # rectified linear\n",
    "                                           learning_rate_init=1e-2,  # learning rate\n",
    "                                           max_iter=1000,            # number of iterations\n",
    "                                           early_stopping=True,      # stop training if validation data gets worse\n",
    "                                           random_state=0)           # random number seed for initialization\n",
    "```\n",
    "\n",
    "Use the MLPClassifier on the `digits` dataset we used in previous assignments.  Split it into 80% training and 20% testing.\n",
    "```\n",
    "import sklearn.datasets\n",
    "digits = sklearn.datasets.load_digits()\n",
    "```\n",
    "\n",
    "Train the classifier on the training data (using `mlp.fit`) and test it on the test data (using `mlp.predict`).\n",
    "\n",
    "Report the accuracy of the classifier, which is computed as the proportion of time that the output is the same as the target output:\n",
    "\n",
    "```np.mean(output == Y_test)```\n",
    "\n",
    "Also generate and print the *confusion matrix*, which is a matrix showing how often particular digits are mistaken for other digits:\n",
    "\n",
    "```\n",
    "confusion = np.zeros((10,10))\n",
    "for i in range(len(output)):\n",
    "    confusion[output[i], Y_test[i]] += 1\n",
    "print(confusion)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) [1 mark]** Repeat the classification in part a) but for different numbers of features (`[5, 10, 15, 20, 25, 30, 35, 40, 45, 50]`).  As with question 1d, repeat the process 10 time for each size, adjusting `random_state` each time.  Generate a plot with the average classification accuracy for these different feature sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) [1 mark]** What happens if you set `hidden_layer_sizes=()`?  This should not generate any new features at all.  How accurate is the system now?  Since there are no new features to learn, what is the MLP doing in this case? (Hint: this is now the same thing as an algorithm we have worked with earlier in the course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) [2 marks]** Using the following dataset, do the best job you can at building a classifier and testing it.\n",
    "\n",
    "```python\n",
    "digits = sklearn.datasets.load_digits()\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    digits.data, digits.target, test_size=0.2, shuffle=True, random_state=0,\n",
    ")\n",
    "```\n",
    "\n",
    "You can use any of the supervised learning models from the assignments so far: the perceptron (`sklearn.linear_model.Perceptron`), regression (`sklearn.linear_model.Ridge`), linear SVM (`sklearn.svm.LinearSVC`), kernel-based SVM ('sklearn.svm.SVC'), and the MLPClassifier (`sklearn.neural_network.MLPClassifier`). Make sure to develop your models only using the training data (perhaps split into training and validation), and only once you have chosen your best model should you test it on the testing data.  You are trying to get the best accuracy (`np.mean(output == Y_test)`) possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 mark]** Describe what you would like to do for your final project.  In particular, tell me what dataset you want to analyse (either one you've made up, or one found online in places like https://www.kaggle.com/datasets or one from the various papers we've discussed in class).  Given that dataset, describe what algorithms you want to try on that dataset.  You should include both very simple algorithms and more complex ones.  Indicate what parameters of those algorithms you would adjust and what you would measure as you are adjusting those parameters.\n",
    "\n",
    "Even though the final project can be done in groups of 2, each member of the group should write their answer this question separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
